{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Impact of Generative AI on Children's Creative Learning Transfer\n",
    "\n",
    "Analysis conducted by Sachin Allums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Qualtrics Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# may need to run pip install pandas first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 286)\n",
      "Index(['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
      "       'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
      "       'RecipientLastName',\n",
      "       ...\n",
      "       'Item3', 'Item4', 'Item5', 'AlternateUsesTask5', 'AssignedCondition',\n",
      "       'alreadyIntroducedToAI', 'StratifiedGroup', 'AlternateUsesTask2',\n",
      "       'AlternateUsesTask3', 'AlternateUsesTask4'],\n",
      "      dtype='object', length=286)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('honors_data_test.csv')\n",
    "print(data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_likert_data(data):\n",
    "    \"\"\"\n",
    "    Transforms all likert questions into numeric scales\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"Strongly agree\": 5,\n",
    "        \"Somewhat agree\": 4,\n",
    "        \"Neither agree nor disagree\": 3,\n",
    "        \"Somewhat disagree\": 2,\n",
    "        \"Strongly disagree\": 1\n",
    "    }\n",
    "    \n",
    "    return data.replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = recode_likert_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creative_self_efficacy(data):\n",
    "    pre_efficacy_ids = [\"Q24_2\", \"Q24_3\", \"Q24_4\", \"Q25_1\", \"Q25_3\", \"Q25_4\"]\n",
    "    post_efficacy_ids = [\"Q42_3\", \"Q42_4\", \"Q42_5\", \"Q43_1\", \"Q43_3\", \"Q43_4\"]\n",
    "\n",
    "    # Convert columns to numeric (coerce non-numeric values to NaN)\n",
    "    data[pre_efficacy_ids + post_efficacy_ids] = data[pre_efficacy_ids + post_efficacy_ids].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Count valid (non-NaN) responses for each participant\n",
    "    pre_valid_counts = data[pre_efficacy_ids].notna().sum(axis=1)\n",
    "    post_valid_counts = data[post_efficacy_ids].notna().sum(axis=1)\n",
    "\n",
    "    # Sum scores while ignoring NaN values\n",
    "    pre_score = data[pre_efficacy_ids].sum(axis=1, skipna=True)\n",
    "    post_score = data[post_efficacy_ids].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Normalize by the available number of responses\n",
    "    pre_score_normalized = (pre_score - pre_valid_counts) / (4 * pre_valid_counts)\n",
    "    post_score_normalized = (post_score - post_valid_counts) / (4 * post_valid_counts)\n",
    "\n",
    "    # Compute difference\n",
    "    data[\"pre_creative_self_efficacy\"] = pre_score_normalized\n",
    "    data[\"post_creative_self_efficacy\"] = post_score_normalized\n",
    "    data[\"difference_creative_self_efficacy\"] = post_score_normalized - pre_score_normalized\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creative_personal_identity(data):\n",
    "    pre_cpi_ids = [\"Q234\", \"Q24_1\", \"Q25_2\", \"Q25_5\", \"Q26_1\"]\n",
    "    post_cpi_ids = [\"Q42_1\", \"Q42_2\", \"Q43_2\", \"Q43_5\", \"Q44_1\"]\n",
    "\n",
    "    # Convert columns to numeric (coerce non-numeric values to NaN)\n",
    "    data[pre_cpi_ids + post_cpi_ids] = data[pre_cpi_ids + post_cpi_ids].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # Count valid (non-NaN) responses for each participant\n",
    "    pre_valid_counts = data[pre_cpi_ids].notna().sum(axis=1)\n",
    "    post_valid_counts = data[post_cpi_ids].notna().sum(axis=1)\n",
    "\n",
    "    # Sum scores while ignoring NaN values\n",
    "    pre_score = data[pre_cpi_ids].sum(axis=1, skipna=True)\n",
    "    post_score = data[post_cpi_ids].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Normalize by the available number of responses\n",
    "    pre_score_normalized = (pre_score - pre_valid_counts) / (4 * pre_valid_counts)\n",
    "    post_score_normalized = (post_score - post_valid_counts) / (4 * post_valid_counts)\n",
    "\n",
    "    # Compute difference\n",
    "    data[\"pre_creative_personal_identity\"] = pre_score_normalized\n",
    "    data[\"post_creative_personal_identity\"] = post_score_normalized\n",
    "    data[\"difference_creative_personal_identity\"] = post_score_normalized - pre_score_normalized\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 292)\n",
      "Index(['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress',\n",
      "       'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId',\n",
      "       'RecipientLastName',\n",
      "       ...\n",
      "       'StratifiedGroup', 'AlternateUsesTask2', 'AlternateUsesTask3',\n",
      "       'AlternateUsesTask4', 'pre_creative_self_efficacy',\n",
      "       'post_creative_self_efficacy', 'difference_creative_self_efficacy',\n",
      "       'pre_creative_personal_identity', 'post_creative_personal_identity',\n",
      "       'difference_creative_personal_identity'],\n",
      "      dtype='object', length=292)\n",
      "0          NaN\n",
      "1          NaN\n",
      "2          NaN\n",
      "3          NaN\n",
      "4          NaN\n",
      "        ...   \n",
      "68    0.666667\n",
      "69    0.750000\n",
      "70    0.916667\n",
      "71    0.708333\n",
      "72         NaN\n",
      "Name: pre_creative_self_efficacy, Length: 73, dtype: float64\n",
      "0          NaN\n",
      "1          NaN\n",
      "2          NaN\n",
      "3          NaN\n",
      "4          NaN\n",
      "        ...   \n",
      "68    0.750000\n",
      "69    0.833333\n",
      "70    0.958333\n",
      "71    0.750000\n",
      "72         NaN\n",
      "Name: post_creative_self_efficacy, Length: 73, dtype: float64\n",
      "0          NaN\n",
      "1          NaN\n",
      "2          NaN\n",
      "3          NaN\n",
      "4          NaN\n",
      "        ...   \n",
      "68    0.083333\n",
      "69    0.083333\n",
      "70    0.041667\n",
      "71    0.041667\n",
      "72         NaN\n",
      "Name: difference_creative_self_efficacy, Length: 73, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = get_creative_self_efficacy(data)\n",
    "data = get_creative_personal_identity(data)\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data[\"pre_creative_self_efficacy\"])\n",
    "print(data[\"post_creative_self_efficacy\"])\n",
    "print(data[\"difference_creative_self_efficacy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_first_AUT_to_embedded_data(df):\n",
    "    \"\"\"\n",
    "    Prepends the Q23 string to AlternateUsesTask1 with a semicolon,\n",
    "    updating the AlternateUsesTask1 column in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'Q23' and 'AlternateUsesTask1' columns.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with updated 'AlternateUsesTask1' column.\n",
    "    \"\"\"\n",
    "    df['AlternateUsesTask1'] = df['Q23'].str.strip() + '; ' + df['AlternateUsesTask1'].str.strip()    \n",
    "    df['AlternateUsesTask5'] = df['Q72'].str.strip() + '; ' + df['AlternateUsesTask5'].str.strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepend_first_AUT_to_embedded_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_alternate_uses(row):\n",
    "    if row['AssignedCondition'] in ['Assisted-First', 'Fully Assisted']:\n",
    "        row['AlternateUsesTask2'] = str(row['Q80']).strip() + '; ' + str(row['AlternateUsesTask2']).strip()\n",
    "        row['AlternateUsesTask3'] = str(row['Q202']).strip() + '; ' + str(row['AlternateUsesTask3']).strip()\n",
    "        row['AlternateUsesTask4'] = str(row['Q210']).strip() + '; ' + str(row['AlternateUsesTask4']).strip()\n",
    "    else:\n",
    "        row['AlternateUsesTask2'] = str(row['Q190']).strip() + '; ' + str(row['AlternateUsesTask2']).strip()\n",
    "        row['AlternateUsesTask3'] = str(row['Q221']).strip() + '; ' + str(row['AlternateUsesTask3']).strip()\n",
    "        row['AlternateUsesTask4'] = str(row['Q229']).strip() + '; ' + str(row['AlternateUsesTask4']).strip()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(prepend_alternate_uses, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Come up with as many alternate ways as possibl...\n",
      "1     {\"ImportId\":\"QID190_TEXT\"}; {\"ImportId\":\"Alter...\n",
      "2                                              nan; nan\n",
      "3                                              nan; nan\n",
      "4                                              nan; nan\n",
      "                            ...                        \n",
      "68    flip it as a game; empty it as use it to grown...\n",
      "69                                        shirt; fabric\n",
      "70    swing; chair; planter; decoration; trampoline;...\n",
      "71    You can attach it to your car; you can make an...\n",
      "72                                             nan; nan\n",
      "Name: AlternateUsesTask2, Length: 73, dtype: object\n",
      "0     Come up with as many alternate ways as possibl...\n",
      "1     {\"ImportId\":\"QID72_TEXT\"}; {\"ImportId\":\"Altern...\n",
      "2                                                   NaN\n",
      "3                                                   NaN\n",
      "4                                                   NaN\n",
      "                            ...                        \n",
      "68    eat off of; play games on; put plants on; stan...\n",
      "69    plastic; utensils; plates; glasses; container;...\n",
      "70    insect killer; dog toy; planter; painting; cat...\n",
      "71    You can wear them.; you can practice embroider...\n",
      "72                                                  NaN\n",
      "Name: AlternateUsesTask5, Length: 73, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['AlternateUsesTask2'])\n",
    "print(data['AlternateUsesTask5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dat_columns(row):\n",
    "    \"\"\"\n",
    "    Assigns DAT1 through DAT10 columns based on the AssignedCondition.\n",
    "    \n",
    "    Parameters:\n",
    "        row (pd.Series): A row of the DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: The row with new DAT columns assigned.\n",
    "    \"\"\"\n",
    "    if row['AssignedCondition'] in ['Control', 'Assisted-First']:\n",
    "        for i in range(1, 11):\n",
    "            row[f'DAT{i}'] = row.get(f'Q37_{i}', None)\n",
    "    elif row['AssignedCondition'] in ['Fully Assisted', 'Assisted-Second']:\n",
    "        for i in range(1, 11):\n",
    "            row[f'DAT{i}'] = row.get(f'Q193_{i}', None)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(assign_dat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        NaN\n",
      "1        NaN\n",
      "2        NaN\n",
      "3        NaN\n",
      "4        NaN\n",
      "       ...  \n",
      "68    flower\n",
      "69       dog\n",
      "70       cat\n",
      "71      Oboe\n",
      "72       NaN\n",
      "Name: DAT1, Length: 73, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['DAT1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_digit_span(row):\n",
    "    \"\"\"\n",
    "    Scores the digit span task based on the highest sequence correctly recalled.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row of the DataFrame containing participant responses.\n",
    "\n",
    "    Returns:\n",
    "        int: The highest digit span score achieved by the participant.\n",
    "    \"\"\"\n",
    "    # Define the correct sequences with their corresponding scores\n",
    "    correct_sequences = {\n",
    "        'Q141': '1376',\n",
    "        'Q143': '95408',\n",
    "        'Q145': '597832',\n",
    "        'Q153': '4012683',\n",
    "        'Q163': '83976574'\n",
    "    }\n",
    "    \n",
    "    # Initialize the score\n",
    "    score = 0\n",
    "    \n",
    "    # Iterate over the correct sequences\n",
    "    for question, correct_answer in correct_sequences.items():\n",
    "        # Check if the participant's response matches the correct answer\n",
    "        if str(row.get(question, '')).strip() == correct_answer:\n",
    "            # Update the score to the length of the correct sequence\n",
    "            score = len(correct_answer)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_span_scores = data.apply(score_digit_span, axis=1)\n",
    "data = pd.concat([data, digit_span_scores.rename('DigitSpanScore')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "68    7\n",
      "69    6\n",
      "70    7\n",
      "71    8\n",
      "72    0\n",
      "Name: DigitSpanScore, Length: 73, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['DigitSpanScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fluency_scores(df):\n",
    "    \"\"\"\n",
    "    Calculates fluency scores for AlternateUsesTask columns by counting semicolons.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing AlternateUsesTask1 to AlternateUsesTask5 columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new fluency columns for each AlternateUsesTask column.\n",
    "    \"\"\"\n",
    "    fluency_frames = {}  # Dictionary to hold new fluency columns\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        col_name = f'AlternateUsesTask{i}'\n",
    "        fluency_col_name = f'Fluency{i}'\n",
    "        fluency_frames[fluency_col_name] = np.where(df[col_name].notna(), df[col_name].str.count(';') + 1, 0)\n",
    "\n",
    "    # Create a DataFrame from the dictionary and concatenate it with the original DataFrame\n",
    "    fluency_df = pd.DataFrame(fluency_frames, index=df.index)\n",
    "    df = pd.concat([df, fluency_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = calculate_fluency_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2.0\n",
      "1     2.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "     ... \n",
      "68    6.0\n",
      "69    8.0\n",
      "70    8.0\n",
      "71    5.0\n",
      "72    0.0\n",
      "Name: Fluency1, Length: 73, dtype: float64\n",
      "Index(['AlternateUsesTask1', 'AlternateUsesTask2', 'AlternateUsesTask3',\n",
      "       'AlternateUsesTask4', 'AlternateUsesTask5', 'AssignedCondition',\n",
      "       'Create New Field or Choose From Dropdown...', 'CreativityLevel',\n",
      "       'DAT1', 'DAT10',\n",
      "       ...\n",
      "       'post_creative_personal_identity', 'post_creative_self_efficacy',\n",
      "       'pre_creative_personal_identity', 'pre_creative_self_efficacy',\n",
      "       'DigitSpanScore', 'Fluency1', 'Fluency2', 'Fluency3', 'Fluency4',\n",
      "       'Fluency5'],\n",
      "      dtype='object', length=308)\n",
      "(73, 308)\n"
     ]
    }
   ],
   "source": [
    "print(data['Fluency1'])\n",
    "print(data.columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45   2025-02-17 21:29:26\n",
      "51   2025-02-19 17:16:03\n",
      "54   2025-02-23 20:00:03\n",
      "55   2025-02-24 16:47:02\n",
      "56   2025-02-19 14:48:31\n",
      "57   2025-02-20 14:02:32\n",
      "58   2025-02-23 14:15:03\n",
      "59   2025-02-25 11:24:13\n",
      "60   2025-03-05 17:31:07\n",
      "61   2025-03-05 17:34:35\n",
      "62   2025-03-09 17:23:59\n",
      "63   2025-03-05 18:22:27\n",
      "64   2025-03-13 18:30:54\n",
      "65   2025-03-14 11:44:15\n",
      "66   2025-03-16 21:14:19\n",
      "67   2025-03-30 17:47:09\n",
      "68   2025-03-30 18:01:44\n",
      "69   2025-03-30 18:41:45\n",
      "70   2025-03-30 19:21:16\n",
      "71   2025-03-31 09:49:38\n",
      "72   2025-03-30 19:36:54\n",
      "Name: StartDate, dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/tw_024nn29x6nr0mwj59pmjw0000gn/T/ipykernel_70410/2386320117.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce').fillna(default_date)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the cutoff date and time\n",
    "cutoff_datetime = pd.to_datetime('2025-02-17 17:00:00')\n",
    "\n",
    "# Step 2: Have a default for any errors\n",
    "default_date = pd.Timestamp('2025-01-01')\n",
    "data['StartDate'] = pd.to_datetime(data['StartDate'], errors='coerce').fillna(default_date)\n",
    "\n",
    "# Step 3: Filter the DataFrame\n",
    "data = data[data['StartDate'] >= cutoff_datetime]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(data['StartDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dat\n",
    "\n",
    "# GloVe model from https://nlp.stanford.edu/projects/glove/\n",
    "model = dat.Model(\"glove.840B.300d.txt\", \"words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.40841\n",
      "78.68301\n",
      "79.78776\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Word examples (Figure 1 in paper)\n",
    "test = [\"Oboe\", \"apricot\" , \"elephant\", \"pillow case\", \"basketball\", \"Vampire\", \"crochet\", \"chess\", \"overalls\", \"tape\"]\n",
    "\n",
    "test2 = [\"dog\", \"graph\", \"skateboard\", \"guitar\", \"shirt\", \"black\", \"window\", \"soup\", \"string lights\", \"pinwheel\"]\n",
    "test3 = [\"cat\", \"vegetable \",\"swimming \",\"sewn\", \"read\", \"watch\", \"blanket\", \"trash\", \"rug\"]\n",
    "# Compute the DAT score (transformed average cosine distance of first 7 valid words)\n",
    "print(model.dat(test)) # 87.03\n",
    "print(model.dat(test2))\n",
    "print(model.dat(test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DAT columns into lists per row\n",
    "dat_cols = [f'DAT{i}' for i in range(1, 11)]\n",
    "\n",
    "# Make sure all words are strings and handle missing values\n",
    "data[dat_cols] = data[dat_cols].astype(str).fillna('')\n",
    "\n",
    "# Apply the DAT model to each row\n",
    "def compute_dat_score(row):\n",
    "    words = [word for word in row[dat_cols] if word]  # Filter out empty strings\n",
    "    return model.dat(words)\n",
    "\n",
    "data['DATScore'] = data.apply(compute_dat_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45    77.898827\n",
      "51    14.901323\n",
      "54    79.355782\n",
      "55    76.684273\n",
      "56          NaN\n",
      "57          NaN\n",
      "58          NaN\n",
      "59          NaN\n",
      "60          NaN\n",
      "61          NaN\n",
      "62    83.678215\n",
      "63          NaN\n",
      "64    77.799004\n",
      "65    82.316826\n",
      "66    82.750648\n",
      "67    80.883568\n",
      "68    89.440758\n",
      "69    78.683006\n",
      "70    79.787758\n",
      "71    87.030182\n",
      "72          NaN\n",
      "Name: DATScore, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['DATScore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
